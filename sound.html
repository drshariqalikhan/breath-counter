<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>Sound Visualizer (Bluetooth Mic Priority)</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f0f0f0;
            color: #333;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        .container {
            background-color: #fff;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            width: 100%;
            max-width: 400px;
            box-sizing: border-box;
        }
        h1 {
            text-align: center;
            color: #1a73e8;
            margin-top: 0;
        }
        button {
            display: block;
            width: 100%;
            padding: 12px;
            background-color: #1a73e8;
            color: white;
            border: none;
            border-radius: 5px;
            font-size: 16px;
            cursor: pointer;
            margin-bottom: 15px;
            transition: background-color 0.3s;
        }
        button:hover:not(:disabled) {
            background-color: #1558b3;
        }
        button:disabled {
            background-color: #ccc;
            cursor: not-allowed;
        }
        #micInfo {
            text-align: center;
            font-size: 12px;
            color: #555;
            margin-bottom: 20px;
            min-height: 1em;
        }
        canvas {
            display: block;
            width: 100%;
            height: 150px;
            background-color: #e9e9e9;
            border-radius: 4px;
            margin-bottom: 20px;
        }
        .logs {
            margin-top: 20px;
            border: 1px solid #ddd;
            padding: 10px;
            height: 150px;
            overflow-y: auto;
            font-size: 12px;
            background-color: #f9f9f9;
            border-radius: 4px;
        }
        .logs p { margin: 2px 0; padding: 2px; border-bottom: 1px solid #eee; }
        .logs .error { color: red; }
        .logs .info { color: green; }
        .logs .warn { color: orange; }
        .logs .debug { color: blue; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Sound Visualizer</h1>

        <button id="startButton">Start Monitoring</button>
        <button id="stopButton" disabled>Stop Monitoring</button>

        <div id="micInfo">Mic: (Inactive)</div>

        <canvas id="soundWaveCanvas"></canvas>
        <p style="text-align:center; font-size: 12px; margin-top: -15px; margin-bottom: 20px;">
            (Overall sound level detected)
        </p>

        <h2>Logs</h2>
        <div id="logsContainer" class="logs">
            <p>App initialized. Waiting for user to start.</p>
        </div>
    </div>

    <script>
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const micInfoDisplay = document.getElementById('micInfo');
        const logsContainer = document.getElementById('logsContainer');
        const canvas = document.getElementById('soundWaveCanvas');
        const canvasCtx = canvas.getContext('2d');

        let audioContext;
        let analyser;
        let microphoneStream;
        let animationFrameId;
        
        const FFT_SIZE = 2048; // Could be smaller now if we don't need high frequency resolution
                               // but AnalyserNode still uses it for its buffer.
        
        const MAX_LOGS = 50;
        const CHART_BUFFER_SIZE = 100;
        let chartData = [];

        function logMessage(message, type = 'info') {
            const p = document.createElement('p');
            const timestamp = new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit', second: '2-digit', fractionalSecondDigits: 3 });
            p.textContent = `[${timestamp}] ${message}`;
            p.className = type;
            logsContainer.appendChild(p);
            if (logsContainer.children.length > MAX_LOGS) {
                logsContainer.removeChild(logsContainer.firstChild);
            }
            logsContainer.scrollTop = logsContainer.scrollHeight;
        }

        async function startMonitoring() {
            logMessage('Attempting to start monitoring...');
            startButton.disabled = true;

            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                logMessage('getUserMedia not supported on your browser!', 'error');
                alert('Your browser does not support microphone access.');
                startButton.disabled = false;
                return;
            }

            let initialStream;
            try {
                initialStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
                logMessage('Initial generic microphone access granted.', 'info');
            } catch (err) {
                logMessage(`Error getting initial microphone permission: ${err.name} - ${err.message}`, 'error');
                alert(`Error accessing microphone: ${err.message}. Please ensure permission is granted.`);
                startButton.disabled = false;
                return;
            }

            let selectedDeviceId = null;
            let selectedDeviceLabel = "Default";

            try {
                const devices = await navigator.mediaDevices.enumerateDevices();
                const audioInputDevices = devices.filter(device => device.kind === 'audioinput');
                logMessage(`Found ${audioInputDevices.length} audio input devices:`, 'debug');
                audioInputDevices.forEach(d => logMessage(`  - Label: "${d.label}", ID: ${d.deviceId.substring(0,10)}...`, 'debug'));

                if (audioInputDevices.length > 0) {
                    let potentialBluetoothDevice = audioInputDevices.find(device => 
                        /bluetooth|headset|airpods/i.test(device.label) && !/iphone microphone|built-in/i.test(device.label)
                    );
                    
                    if (!potentialBluetoothDevice) {
                        potentialBluetoothDevice = audioInputDevices.find(device =>
                           device.label && 
                           !/iphone microphone|built-in/i.test(device.label) && 
                           device.deviceId !== 'default' && 
                           audioInputDevices.length > 1 
                        );
                    }

                    if (potentialBluetoothDevice) {
                        selectedDeviceId = potentialBluetoothDevice.deviceId;
                        selectedDeviceLabel = potentialBluetoothDevice.label || "Selected Non-Default";
                        logMessage(`Attempting to use specific device: ${selectedDeviceLabel} (ID: ${selectedDeviceId.substring(0,10)}...)`, 'info');
                    } else {
                        logMessage('No specific Bluetooth/external headset detected by label, or only one mic. Using system default.', 'warn');
                        if (audioInputDevices.length > 0 && initialStream.getAudioTracks()[0].label) {
                             selectedDeviceLabel = initialStream.getAudioTracks()[0].label || "Default";
                        }
                    }
                } else {
                    logMessage('No audio input devices found after initial grant. This is unexpected.', 'error');
                    if (initialStream) initialStream.getTracks().forEach(track => track.stop());
                    startButton.disabled = false;
                    return;
                }

                if (initialStream) {
                    initialStream.getTracks().forEach(track => track.stop());
                    logMessage('Stopped initial generic stream.', 'debug');
                }

                const constraints = {
                    audio: selectedDeviceId ? { deviceId: { exact: selectedDeviceId } } : true,
                    video: false
                };
                
                logMessage(`Requesting final microphone stream with constraints: ${JSON.stringify(constraints.audio)}`, 'debug');
                microphoneStream = await navigator.mediaDevices.getUserMedia(constraints);
                
                const currentTrack = microphoneStream.getAudioTracks()[0];
                const finalDeviceLabel = currentTrack.label || (selectedDeviceId ? selectedDeviceLabel : "System Default");
                micInfoDisplay.textContent = `Mic: ${finalDeviceLabel.substring(0,30)}${finalDeviceLabel.length > 30 ? '...' : ''}`;
                logMessage(`Successfully using microphone: "${finalDeviceLabel}"`, 'info');
                const settings = currentTrack.getSettings();
                if(settings.deviceId) logMessage(`Actual Device ID being used: ${settings.deviceId.substring(0,10)}...`, 'debug');


            } catch (err) {
                logMessage(`Error during device selection or final stream acquisition: ${err.name} - ${err.message}`, 'error');
                if (initialStream) initialStream.getTracks().forEach(track => track.stop());
                if (microphoneStream) microphoneStream.getTracks().forEach(track => track.stop());
                startButton.disabled = false;
                stopButton.disabled = true;
                micInfoDisplay.textContent = "Mic: Error";
                return;
            }
            
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                logMessage(`AudioContext created. Sample rate: ${audioContext.sampleRate} Hz`, 'info');

                const source = audioContext.createMediaStreamSource(microphoneStream);
                
                analyser = audioContext.createAnalyser();
                analyser.fftSize = FFT_SIZE; 
                analyser.smoothingTimeConstant = 0.5; // Can adjust smoothing for responsiveness

                source.connect(analyser);
                
                logMessage(`Analyser FFT size: ${analyser.fftSize}, Freq bin count: ${analyser.frequencyBinCount}`, 'info');
                
                stopButton.disabled = false;
                chartData = [];
                
                processAudio();

            } catch (err) {
                logMessage(`Error setting up AudioContext/Analyser: ${err.message}`, 'error');
                if (microphoneStream) microphoneStream.getTracks().forEach(track => track.stop());
                startButton.disabled = false;
                stopButton.disabled = true;
                micInfoDisplay.textContent = "Mic: Error";
            }
        }

        function stopMonitoring() {
            logMessage('Stopping monitoring...');
            if (animationFrameId) {
                cancelAnimationFrame(animationFrameId);
                animationFrameId = null;
            }
            if (microphoneStream) {
                microphoneStream.getTracks().forEach(track => track.stop());
                logMessage('Microphone stream stopped.', 'info');
                microphoneStream = null;
            }
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.close().then(() => logMessage('AudioContext closed.', 'info'));
                audioContext = null;
            }
            
            startButton.disabled = false;
            stopButton.disabled = true;
            chartData = []; 
            drawChart(); 
            micInfoDisplay.textContent = "Mic: (Inactive)";
        }

        function processAudio() {
            if (!analyser) {
                logMessage('Analyser not ready in processAudio', 'error');
                stopMonitoring();
                return;
            }
            const bufferLength = analyser.frequencyBinCount; // This is fftSize / 2
            const dataArray = new Uint8Array(bufferLength); 

            analyser.getByteFrequencyData(dataArray); // Fills dataArray with frequency data (0-255 per bin)

            // Calculate overall sound level: average of all frequency bin values
            let sumEnergy = 0;
            for (let i = 0; i < bufferLength; i++) {
                sumEnergy += dataArray[i];
            }
            const overallAverageEnergy = bufferLength > 0 ? sumEnergy / bufferLength : 0;
            
            // Add to chart data (the average energy, which will be 0-255)
            chartData.push(overallAverageEnergy);
            if (chartData.length > CHART_BUFFER_SIZE) {
                chartData.shift();
            }
            drawChart();
            
            animationFrameId = requestAnimationFrame(processAudio);
        }
        
        function drawChart() {
            canvasCtx.fillStyle = '#e9e9e9';
            canvasCtx.fillRect(0, 0, canvas.width, canvas.height);

            if (chartData.length === 0) return;

            canvasCtx.lineWidth = 2;
            canvasCtx.strokeStyle = '#1a73e8';
            canvasCtx.beginPath();

            const sliceWidth = canvas.width * 1.0 / chartData.length;
            let x = 0;

            for (let i = 0; i < chartData.length; i++) {
                // dataArray values are 0-255. overallAverageEnergy will also be in this range.
                // Normalize to 0-1 for charting.
                const v = chartData[i] / 255.0; 
                const y = canvas.height - (v * canvas.height);

                if (i === 0) {
                    canvasCtx.moveTo(x, y);
                } else {
                    canvasCtx.lineTo(x, y);
                }
                x += sliceWidth;
            }
            canvasCtx.stroke();
        }
        
        function resizeCanvas() {
            const displayWidth  = canvas.clientWidth;
            const displayHeight = canvas.clientHeight;

            if (canvas.width  !== displayWidth || canvas.height !== displayHeight) {
                canvas.width  = displayWidth;
                canvas.height = displayHeight;
                drawChart();
            }
        }

        startButton.addEventListener('click', startMonitoring);
        stopButton.addEventListener('click', stopMonitoring);
        window.addEventListener('resize', resizeCanvas);

        resizeCanvas();
        logMessage('App ready. Connect Bluetooth headset if desired, then click "Start Monitoring".', 'info');

    </script>
</body>
</html>
